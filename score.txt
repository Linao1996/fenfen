# 3.4 with HMM，自己的训练数据 pku
# TOTAL TRUE WORDS RECALL:	0.780
# TOTAL TEST WORDS PRECISION:	0.854
# F MEASURE:	0.815


# 3.4 with HMM， jieba训练数据 pku
# === TOTAL TRUE WORDS RECALL:	0.796
# === TOTAL TEST WORDS PRECISION:	0.861
# === F MEASURE:	0.827

# 3.4 without HMM， jieba训练数据 pku
# === TOTAL TRUE WORDS RECALL:	0.811
# === TOTAL TEST WORDS PRECISION:	0.864
# === F MEASURE:	0.836

# 3.5 without HMM 自己的词典
# === TOTAL TRUE WORDS RECALL:	0.864
# === TOTAL TEST WORDS PRECISION:	0.874
# === F MEASURE:	0.869


# 3.5 without HMM 自己的词典 renmin
# === TOTAL TRUE WORDS RECALL:	0.923
# === TOTAL TEST WORDS PRECISION:	0.893
# === F MEASURE:	0.908
# !!!!可见词典的重要性，使用MSR测试数据F值第1次到达90

# 3.6 纯HMM 6-tag 训练数据：人民日报 pku结果
# === TOTAL TRUE WORDS RECALL:	0.805
# === TOTAL TEST WORDS PRECISION:	0.790
# === F MEASURE:	0.798

# 自己的HMM训练数据始终不能识别人名，除非把emit换成jieba的数据,怀疑jieba作者对emit做了手脚
# 向训练数据添加大量人名，使得人名识别能力上升，但bakeoff表现下降很多

# 隐马模型一个最大的缺点就是由于其输出独立性假设，导致其不能考虑上下文的特征，限制了特征的选择
# 最大熵隐马模型则解决了隐马的问题，可以任意选择特征，但由于其在每一节点都要进行归一化，所以只能找到局部的最优值，同时也带来了标记偏见的问题，即凡是训练语料中未出现的情况全都忽略掉
# 条件随机场则很好的解决了这一问题，他并不在每一个节点进行归一化，而是所有特征进行全局归一化，因此可以求得全局的最优值。